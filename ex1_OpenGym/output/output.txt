Step 0: Action 0, Observation [-0.04838955 -0.20220433 -0.0246807   0.2526515 ], Reward 1.0, Done False, Truncated False
Step 1: Action 0, Observation [-0.05243364 -0.39696532 -0.01962767  0.53744864], Reward 1.0, Done False, Truncated False
Step 2: Action 0, Observation [-0.06037294 -0.5918059  -0.00887869  0.82388306], Reward 1.0, Done False, Truncated False
Step 3: Action 1, Observation [-0.07220906 -0.39656362  0.00759897  0.52842087], Reward 1.0, Done False, Truncated False
.
.
.
Step 17: Action 1, Observation [-0.18088649 -0.4134536   0.18452047  0.9150959 ], Reward 1.0, Done False, Truncated False
Step 18: Action 0, Observation [-0.18915556 -0.61052686  0.20282239  1.2596294 ], Reward 1.0, Done False, Truncated False
Step 19: Action 1, Observation [-0.20136611 -0.4184929   0.22801498  1.0367047 ], Reward 1.0, Done True, Truncated False
Episode finished after 20 timesteps


(py38) S:\dev\reinforcement_learning>python ex1_OpenGym\basic_cart_pole.py
S:\dev\env\envs\py38\lib\site-packages\gym\utils\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)
  if not isinstance(terminated, (bool, np.bool8)):
Step 0: Action 1, Observation [ 0.04366344  0.20371236 -0.00680198 -0.32023627], Reward 1.0, Done False, Truncated False
Step 1: Action 1, Observation [ 0.04773769  0.39893052 -0.01320671 -0.61505646], Reward 1.0, Done False, Truncated False
Step 2: Action 1, Observation [ 0.0557163   0.59423447 -0.02550784 -0.9118695 ], Reward 1.0, Done False, Truncated False
Step 3: Action 1, Observation [ 0.06760099  0.7896921  -0.04374523 -1.2124591 ], Reward 1.0, Done False, Truncated False
Step 4: Action 0, Observation [ 0.08339483  0.5951612  -0.06799441 -0.9337989 ], Reward 1.0, Done False, Truncated False
Step 5: Action 1, Observation [ 0.09529806  0.7911313  -0.08667038 -1.2470498 ], Reward 1.0, Done False, Truncated False
Step 6: Action 0, Observation [ 0.11112068  0.59722114 -0.11161138 -0.98272544], Reward 1.0, Done False, Truncated False
Step 7: Action 1, Observation [ 0.12306511  0.7936472  -0.1312659  -1.308277  ], Reward 1.0, Done False, Truncated False
Step 8: Action 0, Observation [ 0.13893805  0.6004099  -0.15743142 -1.0593961 ], Reward 1.0, Done False, Truncated False
Step 9: Action 1, Observation [ 0.15094624  0.7972269  -0.17861935 -1.3970644 ], Reward 1.0, Done False, Truncated False
Step 10: Action 1, Observation [ 0.16689079  0.99406344 -0.20656064 -1.7398571 ], Reward 1.0, Done False, Truncated False
Step 11: Action 1, Observation [ 0.18677205  1.1908528  -0.24135779 -2.0890641 ], Reward 1.0, Done True, Truncated False
Episode finished after 12 timesteps
