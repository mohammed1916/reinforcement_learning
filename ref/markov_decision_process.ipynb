{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36b9d381",
   "metadata": {},
   "source": [
    "# Markov Decision Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dde047bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial state is 1\n",
      "From state 1 to state 2 with action 0, reward: 1\n",
      "From state 2 to state 1 with action 0, reward: 0\n",
      "From state 1 to state 3 with action 1, reward: 10\n",
      "From state 3 to state 3 with action 1, reward: 10\n",
      "From state 3 to state 2 with action 0, reward: 0\n",
      "From state 2 to state 3 with action 1, reward: 1\n"
     ]
    }
   ],
   "source": [
    "#Markov Decision Process\n",
    "\n",
    "from typing import Tuple\n",
    "\n",
    "class Environment:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Constructor of the Environment class.\n",
    "        \"\"\"\n",
    "        self._initial_state = 1\n",
    "        self._allowed_actions = [0, 1]  # 0: A, 1: B\n",
    "        self._states = [1, 2, 3]\n",
    "        self._current_state = self._initial_state\n",
    "\n",
    "    def step(self, action: int) -> Tuple[int, int]:\n",
    "        \"\"\"\n",
    "        Step function: compute the one-step dynamic from the given action.\n",
    "\n",
    "        Args:\n",
    "            action (int): the action taken by the agent.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[int, int]: The tuple (current_state, reward).\n",
    "        \"\"\"\n",
    "\n",
    "        # check if the action is allowed\n",
    "        if action not in self._allowed_actions:\n",
    "            raise ValueError(\"Action is not allowed\")\n",
    "\n",
    "        reward = 0\n",
    "        if action == 0 and self._current_state == 1:\n",
    "            self._current_state = 2\n",
    "            reward = 1\n",
    "        elif action == 1 and self._current_state == 1:\n",
    "            self._current_state = 3\n",
    "            reward = 10\n",
    "        elif action == 0 and self._current_state == 2:\n",
    "            self._current_state = 1\n",
    "            reward = 0\n",
    "        elif action == 1 and self._current_state == 2:\n",
    "            self._current_state = 3\n",
    "            reward = 1\n",
    "        elif action == 0 and self._current_state == 3:\n",
    "            self._current_state = 2\n",
    "            reward = 0\n",
    "        elif action == 1 and self._current_state == 3:\n",
    "            self._current_state = 3\n",
    "            reward = 10\n",
    "\n",
    "        return self._current_state, reward\n",
    "\n",
    "    def reset(self) -> int:\n",
    "        \"\"\"\n",
    "        Reset the environment starting from the initial state.\n",
    "\n",
    "        Returns:\n",
    "            int: The environment state after reset (initial state).\n",
    "        \"\"\"\n",
    "        self._current_state = self._initial_state\n",
    "        return self._current_state\n",
    "\n",
    "# Instantiate the environment and run some actions\n",
    "env = Environment()\n",
    "state = env.reset()\n",
    "\n",
    "actions = [0, 0, 1, 1, 0, 1]\n",
    "\n",
    "print(f\"Initial state is {state}\")\n",
    "\n",
    "for action in actions:\n",
    "    next_state, reward = env.step(action)\n",
    "    print(f\"From state {state} to state {next_state} with action {action}, reward: {reward}\")\n",
    "    state = next_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11a20ec8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transition Matrix:\n",
      " [[0.3 0.7]\n",
      " [0.2 0.8]]\n",
      "\n",
      "Iter 0. Probability vector s = [0.25 0.75]\n",
      "\n",
      "Iter 1. Probability vector s = [0.225 0.775]\n",
      "\n",
      "Iter 2. Probability vector s = [0.2225 0.7775]\n",
      "\n",
      "Iter 3. Probability vector s = [0.22225 0.77775]\n",
      "\n",
      "Iter 4. Probability vector s = [0.222225 0.777775]\n",
      "\n",
      "Iter 5. Probability vector s = [0.2222225 0.7777775]\n",
      "\n",
      "Iter 6. Probability vector s = [0.22222225 0.77777775]\n",
      "\n",
      "Iter 7. Probability vector s = [0.22222223 0.77777778]\n",
      "\n",
      "Iter 8. Probability vector s = [0.22222222 0.77777778]\n",
      "\n",
      "Iter 9. Probability vector s = [0.22222222 0.77777778]\n",
      "\n",
      "Final Vector s=[0.22222222 0.77777778]\n"
     ]
    }
   ],
   "source": [
    "#Markov Decision Process\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "p = np.array([[0.3, 0.7],[0.2, 0.8]])\n",
    "print(\"Transition Matrix:\\n\", p)\n",
    "s = np.array([0.5, 0.5])\n",
    "for i in range(10):\n",
    "    s = np.dot(s, p)\n",
    "    print(\"\\nIter {0}. Probability vector s = {1}\".format(i,s))\n",
    "print(\"\\nFinal Vector s={0}\".format(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ce837d",
   "metadata": {},
   "source": [
    "# Markov Reward Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9694611a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: Distracted\n",
      "  Transition to Distracted with probability 0.9 and reward 0\n",
      "  Transition to Study with probability 0.1 and reward -3\n",
      "State: Study\n",
      "  Transition to Distracted with probability 0.5 and reward 0\n",
      "  Transition to Study with probability 0.3 and reward -3\n",
      "  Transition to Take Exam with probability 0.2 and reward -2\n",
      "State: Take Exam\n",
      "  Transition to Study with probability 0.4 and reward -2\n",
      "  Transition to Obtain Certificate with probability 0.6 and reward 10\n",
      "State: Obtain Certificate\n"
     ]
    }
   ],
   "source": [
    "# Define the states\n",
    "states = [\"Distracted\", \"Study\", \"Take Exam\", \"Obtain Certificate\"]\n",
    "\n",
    "# Define the transitions with probabilities and rewards\n",
    "transitions = {\n",
    "    \"Distracted\": [(\"Distracted\", 0.9, 0), (\"Study\", 0.1, -3)],\n",
    "    \"Study\": [(\"Distracted\", 0.5, 0), (\"Study\", 0.3, -3), (\"Take Exam\", 0.2, -2)],\n",
    "    \"Take Exam\": [(\"Study\", 0.4, -2), (\"Obtain Certificate\", 0.6, 10)],\n",
    "    \"Obtain Certificate\": []\n",
    "}\n",
    "\n",
    "# Print the state variables\n",
    "for state, transitions_list in transitions.items():\n",
    "    print(f\"State: {state}\")\n",
    "    for transition in transitions_list:\n",
    "        print(f\"  Transition to {transition[0]} with probability {transition[1]} and reward {transition[2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2479d3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transition Matrix:\n",
      "\t\t\t           Distracted                Study            Take Exam   Obtain Certificate\n",
      "          Distracted                 0.90                 0.10                 0.00                 0.00 \n",
      "               Study                 0.50                 0.30                 0.20                 0.00 \n",
      "           Take Exam                 0.00                 0.40                 0.00                 0.60 \n",
      "  Obtain Certificate                 0.00                 0.00                 0.00                 0.00 \n",
      "\n",
      "Reward Matrix:\n",
      "\t\t\t           Distracted                Study            Take Exam   Obtain Certificate\n",
      "          Distracted                 0.00                -3.00                 0.00                 0.00 \n",
      "               Study                 0.00                -3.00                -2.00                 0.00 \n",
      "           Take Exam                 0.00                -2.00                 0.00                10.00 \n",
      "  Obtain Certificate                 0.00                 0.00                 0.00                 0.00 \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the states\n",
    "states = [\"Distracted\", \"Study\", \"Take Exam\", \"Obtain Certificate\"]\n",
    "\n",
    "# Define the transitions with probabilities and rewards\n",
    "transitions = {\n",
    "    \"Distracted\": [(\"Distracted\", 0.9, 0), (\"Study\", 0.1, -3)],\n",
    "    \"Study\": [(\"Distracted\", 0.5, 0), (\"Study\", 0.3, -3), (\"Take Exam\", 0.2, -2)],\n",
    "    \"Take Exam\": [(\"Study\", 0.4, -2), (\"Obtain Certificate\", 0.6, 10)],\n",
    "    \"Obtain Certificate\": []\n",
    "}\n",
    "\n",
    "# Create dictionaries for indexing\n",
    "state_index = {state: i for i, state in enumerate(states)}\n",
    "\n",
    "# Initialize matrices\n",
    "transition_matrix = np.zeros((len(states), len(states)))\n",
    "reward_matrix = np.zeros((len(states), len(states)))\n",
    "\n",
    "# Populate the matrices\n",
    "for state, transitions_list in transitions.items():\n",
    "    i = state_index[state]\n",
    "    for next_state, prob, reward in transitions_list:\n",
    "        j = state_index[next_state]\n",
    "        transition_matrix[i, j] = prob\n",
    "        reward_matrix[i, j] = reward\n",
    "\n",
    "# Print the transition matrix\n",
    "print(\"Transition Matrix:\")\n",
    "print(\"\\t\\t\\t \" + \" \".join(f\"{state:>20}\" for state in states))\n",
    "for i, state in enumerate(states):\n",
    "    print(f\"{state:>20}\", end=\" \")\n",
    "    for j in range(len(states)):\n",
    "        print(f\"{transition_matrix[i, j]:>20.2f}\", end=\" \")\n",
    "    print()\n",
    "\n",
    "# Print the reward matrix\n",
    "print(\"\\nReward Matrix:\")\n",
    "print(\"\\t\\t\\t \" + \" \".join(f\"{state:>20}\" for state in states))\n",
    "for i, state in enumerate(states):\n",
    "    print(f\"{state:>20}\", end=\" \")\n",
    "    for j in range(len(states)):\n",
    "        print(f\"{reward_matrix[i, j]:>20.2f}\", end=\" \")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db2b120e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total reward for the path ['Study', 'Distracted', 'Take Exam', 'Obtain Certificate'] is: 10\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the states and transitions\n",
    "states = [\"Distracted\", \"Study\", \"Take Exam\", \"Obtain Certificate\"]\n",
    "\n",
    "transitions = {\n",
    "    \"Distracted\": [(\"Distracted\", 0.9, 0), (\"Study\", 0.1, -3)],\n",
    "    \"Study\": [(\"Distracted\", 0.5, 0), (\"Study\", 0.3, -3), (\"Take Exam\", 0.2, -2)],\n",
    "    \"Take Exam\": [(\"Study\", 0.4, -2), (\"Obtain Certificate\", 0.6, 10)],\n",
    "    \"Obtain Certificate\": []\n",
    "}\n",
    "\n",
    "def get_transition_reward(current_state, next_state):\n",
    "    transitions_list = transitions.get(current_state, [])\n",
    "    for state, prob, reward in transitions_list:\n",
    "        if state == next_state:\n",
    "            return reward\n",
    "    return 0  # Return 0 if no transition exists (shouldn't happen if path is valid)\n",
    "\n",
    "def calculate_total_reward(path):\n",
    "    total_reward = 0\n",
    "    for i in range(len(path) - 1):\n",
    "        current_state = path[i]\n",
    "        next_state = path[i + 1]\n",
    "        reward = get_transition_reward(current_state, next_state)\n",
    "        total_reward += reward\n",
    "    \n",
    "    return total_reward\n",
    "\n",
    "# Example path\n",
    "path = [\"Study\", \"Distracted\",\"Take Exam\", \"Obtain Certificate\"]\n",
    "\n",
    "# Calculate and print the total reward for the path\n",
    "total_reward = calculate_total_reward(path)\n",
    "print(f\"The total reward for the path {path} is: {total_reward}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0a539864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total reward for the path ['Distracted', 'Study', 'Take Exam', 'Obtain Certificate'] is: 3.3000000000000016\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the states and transitions\n",
    "states = [\"Distracted\", \"Study\", \"Take Exam\", \"Obtain Certificate\"]\n",
    "\n",
    "transitions = {\n",
    "    \"Distracted\": [(\"Distracted\", 0.9, 0), (\"Study\", 0.1, -3)],\n",
    "    \"Study\": [(\"Distracted\", 0.5, 0), (\"Study\", 0.3, -3), (\"Take Exam\", 0.2, -2)],\n",
    "    \"Take Exam\": [(\"Study\", 0.4, -2), (\"Obtain Certificate\", 0.6, 10)],\n",
    "    \"Obtain Certificate\": []\n",
    "}\n",
    "\n",
    "def get_transition_reward(current_state, next_state):\n",
    "    transitions_list = transitions.get(current_state, [])\n",
    "    for state, prob, reward in transitions_list:\n",
    "        if state == next_state:\n",
    "            return reward\n",
    "    return 0  # Return 0 if no transition exists (shouldn't happen if path is valid)\n",
    "\n",
    "def calculate_total_reward(path, gamma):\n",
    "    total_reward = 0\n",
    "    for i in range(len(path) - 1):\n",
    "        current_state = path[i]\n",
    "        next_state = path[i + 1]\n",
    "        reward = get_transition_reward(current_state, next_state)\n",
    "        total_reward += ((0.9)**i)*reward\n",
    "    \n",
    "    return total_reward\n",
    "\n",
    "# Example path\n",
    "path = [\"Distracted\", \"Study\", \"Take Exam\", \"Obtain Certificate\"]\n",
    "gamma = 0.9\n",
    "    \n",
    "# Calculate and print the total reward for the path\n",
    "total_reward = calculate_total_reward(path, gamma)\n",
    "print(f\"The total reward for the path {path} is: {total_reward}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fc42249c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State history: ['Distracted', 'Distracted', 'Distracted', 'Study', 'Distracted']\n",
      "Total reward: -6\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the states\n",
    "states = [\"Distracted\", \"Study\", \"Take exam\", \"Obtain Certificate\"]\n",
    "\n",
    "# Define the transition probabilities matrix\n",
    "P = np.array([\n",
    "    [0.9, 0.1, 0.0, 0.0],  # Distracted -> [Distracted, Study, Take Exam, Obtain Certificate]\n",
    "    [0.3, 0.5, 0.2, 0.0],  # Study -> [Distracted, Study, Take Exam, Obtain Certificate]\n",
    "    [0.0, 0.4, 0.0, 0.6],  # Take Exam -> [Distracted, Study, Take Exam, Obtain Certificate]\n",
    "    [0.0, 0.0, 0.0, 1.0]   # Obtain Certificate -> [Distracted, Study, Take Exam, Obtain Certificate]\n",
    "])\n",
    "\n",
    "# Define the reward matrix corresponding to each transition\n",
    "R= np.array([\n",
    "    [0, -3, 0, 0],    # Rewards for transitions from Distracted\n",
    "    [-3, -2, -2, 0],    # Rewards for transitions from Study\n",
    "    [0, 0, 0, 10],    # Rewards for transitions from Take Exam\n",
    "    [0, 0, 0, 0]      # Rewards for transitions from Obtain Certificate\n",
    "])\n",
    "def simulate_markov(initial_state, P, R, steps=4):\n",
    "    current_state = initial_state\n",
    "    total_reward = 0\n",
    "    state_history = [states[current_state]]\n",
    "\n",
    "    for _ in range(steps):\n",
    "        next_state = np.random.choice(len(states), p=P[current_state])\n",
    "        reward = R[current_state, next_state]\n",
    "        total_reward += reward\n",
    "        state_history.append(states[next_state])\n",
    "        current_state = next_state\n",
    "\n",
    "        # If reached terminal state\n",
    "        if current_state == len(states) - 1:\n",
    "            break\n",
    "\n",
    "    return state_history, total_reward\n",
    "\n",
    "# Initial state: Distracted (index 0)\n",
    "initial_state = 0\n",
    "state_history, total_reward = simulate_markov(initial_state, P, R)\n",
    "\n",
    "print(\"State history:\", state_history)\n",
    "print(\"Total reward:\", total_reward)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
