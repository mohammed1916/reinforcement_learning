{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: Action 0, Observation [-0.04838955 -0.20220433 -0.0246807   0.2526515 ], Reward 1.0, Done False, Truncated False\n",
      "Step 1: Action 0, Observation [-0.05243364 -0.39696532 -0.01962767  0.53744864], Reward 1.0, Done False, Truncated False\n",
      "Step 2: Action 0, Observation [-0.06037294 -0.5918059  -0.00887869  0.82388306], Reward 1.0, Done False, Truncated False\n",
      "Step 3: Action 1, Observation [-0.07220906 -0.39656362  0.00759897  0.52842087], Reward 1.0, Done False, Truncated False\n",
      "Step 4: Action 0, Observation [-0.08014034 -0.5917916   0.01816738  0.82348853], Reward 1.0, Done False, Truncated False\n",
      "Step 5: Action 0, Observation [-0.09197617 -0.78715736  0.03463716  1.1218296 ], Reward 1.0, Done False, Truncated False\n",
      "Step 6: Action 1, Observation [-0.10771932 -0.59250635  0.05707375  0.84020954], Reward 1.0, Done False, Truncated False\n",
      "Step 7: Action 1, Observation [-0.11956944 -0.39820808  0.07387794  0.5660073 ], Reward 1.0, Done False, Truncated False\n",
      "Step 8: Action 1, Observation [-0.1275336  -0.20419608  0.08519809  0.29748347], Reward 1.0, Done False, Truncated False\n",
      "Step 9: Action 1, Observation [-0.13161753 -0.01038543  0.09114776  0.03283992], Reward 1.0, Done False, Truncated False\n",
      "Step 10: Action 0, Observation [-0.13182524 -0.20668824  0.09180456  0.3528322 ], Reward 1.0, Done False, Truncated False\n",
      "Step 11: Action 0, Observation [-0.135959   -0.40298757  0.0988612   0.6729939 ], Reward 1.0, Done False, Truncated False\n",
      "Step 12: Action 1, Observation [-0.14401875 -0.20936862  0.11232107  0.41300142], Reward 1.0, Done False, Truncated False\n",
      "Step 13: Action 0, Observation [-0.14820613 -0.4058886   0.12058111  0.7388769 ], Reward 1.0, Done False, Truncated False\n",
      "Step 14: Action 1, Observation [-0.1563239  -0.2126198   0.13535865  0.48644367], Reward 1.0, Done False, Truncated False\n",
      "Step 15: Action 0, Observation [-0.16057628 -0.40936604  0.14508751  0.81853765], Reward 1.0, Done False, Truncated False\n",
      "Step 16: Action 0, Observation [-0.16876361 -0.6061442   0.16145827  1.1531099 ], Reward 1.0, Done False, Truncated False\n",
      "Step 17: Action 1, Observation [-0.18088649 -0.4134536   0.18452047  0.9150959 ], Reward 1.0, Done False, Truncated False\n",
      "Step 18: Action 0, Observation [-0.18915556 -0.61052686  0.20282239  1.2596294 ], Reward 1.0, Done False, Truncated False\n",
      "Step 19: Action 1, Observation [-0.20136611 -0.4184929   0.22801498  1.0367047 ], Reward 1.0, Done True, Truncated False\n",
      "Episode finished after 20 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kumar\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gym\\envs\\classic_control\\cartpole.py:211: UserWarning: \u001b[33mWARN: You are calling render method without specifying any render mode. You can specify the render_mode at initialization, e.g. gym(\"CartPole-v1\", render_mode=\"rgb_array\")\u001b[0m\n",
      "  gym.logger.warn(\n",
      "c:\\Users\\kumar\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "# Create the environment\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "# Reset the environment to the initial state\n",
    "observation = env.reset()\n",
    "\n",
    "for t in range(1000):\n",
    "    # Render the environment (this will show a pop-up window)\n",
    "    env.render()\n",
    "    \n",
    "    # Take a random action\n",
    "    action = env.action_space.sample()\n",
    "    \n",
    "    # Apply the action to the environment\n",
    "    observation, reward, done, truncated, info = env.step(action)\n",
    "    \n",
    "    # Print the step information\n",
    "    print(f\"Step {t}: Action {action}, Observation {observation}, Reward {reward}, Done {done}, Truncated {truncated}\")\n",
    "    \n",
    "    # Check if the episode is finished\n",
    "    if done or truncated:\n",
    "        print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "        break\n",
    "\n",
    "# Close the environment\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CartPole-v0 CartPole-v1 MountainCar-v0 MountainCarContinuous-v0 Pendulum-v1 Acrobot-v1 LunarLander-v2 LunarLanderContinuous-v2 BipedalWalker-v3 BipedalWalkerHardcore-v3 CarRacing-v2 Blackjack-v1 FrozenLake-v1 FrozenLake8x8-v1 CliffWalking-v0 Taxi-v3 Reacher-v2 Reacher-v4 Pusher-v2 Pusher-v4 InvertedPendulum-v2 InvertedPendulum-v4 InvertedDoublePendulum-v2 InvertedDoublePendulum-v4 HalfCheetah-v2 HalfCheetah-v3 HalfCheetah-v4 Hopper-v2 Hopper-v3 Hopper-v4 Swimmer-v2 Swimmer-v3 Swimmer-v4 Walker2d-v2 Walker2d-v3 Walker2d-v4 Ant-v2 Ant-v3 Ant-v4 Humanoid-v2 Humanoid-v3 Humanoid-v4 HumanoidStandup-v2 HumanoidStandup-v4\n"
     ]
    }
   ],
   "source": [
    "# 2. To check all env available, uninstalled ones are also shown.\n",
    "from gym import envs \n",
    "print(*envs.registry.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([-0.40538502,  0.        ], dtype=float32), {})\n",
      "[-0.4074545  -0.00206947] -0.06414223554040213 False {}\n",
      "[-0.4074545  -0.00206947]\n",
      "[-0.4115601  -0.00410561] -0.06215519010448248 False {}\n",
      "[-0.4115601  -0.00410561]\n",
      "[-0.41723934 -0.00567926] -0.024939808760220308 False {}\n",
      "[-0.41723934 -0.00567926]\n",
      "[-0.4236572  -0.00641787] -9.249034326569463e-05 False {}\n",
      "[-0.4236572  -0.00641787]\n",
      "[-0.43163636 -0.00797916] -0.03009643341168875 False {}\n",
      "[-0.43163636 -0.00797916]\n",
      "[-0.43944326 -0.0078069 ] -0.0323577755555803 False {}\n",
      "[-0.43944326 -0.0078069 ]\n",
      "[-0.44660357 -0.00716031] -0.07180570773533682 False {}\n",
      "[-0.44660357 -0.00716031]\n",
      "[-0.45286587 -0.00626229] -0.09608749901459 False {}\n",
      "[-0.45286587 -0.00626229]\n",
      "[-0.46114522 -0.00827935] -0.09874207913526334 False {}\n",
      "[-0.46114522 -0.00827935]\n",
      "[-0.46874896 -0.00760373] -0.057890126708855766 False {}\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make('MountainCarContinuous-v0') # try for different environments\n",
    "observation = env.reset()\n",
    "for t in range(10):\n",
    "        env.render()\n",
    "        print(observation)\n",
    "        action = env.action_space.sample()\n",
    "        observation, reward, done, truncated, info = env.step(action)\n",
    "        print(observation, reward, done, info)\n",
    "        if done:\n",
    "            print(\"Finished after {} timesteps\".format(t+1))\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
