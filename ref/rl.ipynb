{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Reward: 642\n",
      "Estimated Values of Arms: [0.08064516 0.56962025 0.70468187 0.19230769]\n",
      "Counts of Arms Pulled: [ 62.  79. 833.  26.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class EpsilonGreedyBandit:\n",
    "    def __init__(self, n_arms, epsilon, true_rewards):\n",
    "        self.n_arms = n_arms\n",
    "        self.epsilon = epsilon\n",
    "        self.true_rewards = true_rewards  # True mean rewards for each arm\n",
    "        self.arm_counts = np.zeros(n_arms)  # Count of times each arm was pulled\n",
    "        self.arm_values = np.zeros(n_arms)  # Estimated value of each arm\n",
    "    \n",
    "    def select_arm(self):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            # Exploration: choose a random arm\n",
    "            return np.random.randint(self.n_arms)\n",
    "        else:\n",
    "            # Exploitation: choose the arm with the highest estimated value\n",
    "            return np.argmax(self.arm_values)\n",
    "    \n",
    "    def update_estimates(self, chosen_arm, reward):\n",
    "        # Increment the count of the chosen arm\n",
    "        self.arm_counts[chosen_arm] += 1\n",
    "        \n",
    "        # Update the estimated value of the chosen arm using incremental formula\n",
    "        n = self.arm_counts[chosen_arm]\n",
    "        value = self.arm_values[chosen_arm]\n",
    "        # Update rule: Q_new = Q_old + (reward - Q_old) / n\n",
    "        self.arm_values[chosen_arm] = value + (reward - value) / n\n",
    "    \n",
    "    def run(self, n_steps):\n",
    "        total_reward = 0\n",
    "        rewards = np.zeros(n_steps)\n",
    "        \n",
    "        for step in range(n_steps):\n",
    "            # Select an arm using epsilon-greedy strategy\n",
    "            chosen_arm = self.select_arm()\n",
    "            \n",
    "            # Simulate the reward for the chosen arm based on the true reward distribution\n",
    "            reward = np.random.binomial(1, self.true_rewards[chosen_arm])\n",
    "            \n",
    "            # Update the estimates for the chosen arm\n",
    "            self.update_estimates(chosen_arm, reward)\n",
    "            \n",
    "            # Update total reward and store reward of this step\n",
    "            total_reward += reward\n",
    "            rewards[step] = reward\n",
    "        \n",
    "        return total_reward, rewards\n",
    "\n",
    "# Parameters\n",
    "n_arms = 5                    # Number of arms\n",
    "epsilon = 0.1                 # Exploration probability\n",
    "true_rewards = [0.1, 0.5, 0.7, 0.3, 0.9]  # True probabilities of reward for each arm\n",
    "n_steps = 1000                # Number of steps\n",
    "\n",
    "# Create an instance of the bandit with epsilon-greedy strategy\n",
    "bandit = EpsilonGreedyBandit(n_arms, epsilon, true_rewards)\n",
    "\n",
    "# Run the bandit algorithm for the specified number of steps\n",
    "total_reward, rewards = bandit.run(n_steps)\n",
    "\n",
    "# Output the results\n",
    "print(f\"Total Reward: {total_reward}\")\n",
    "print(f\"Estimated Values of Arms: {bandit.arm_values}\")\n",
    "print(f\"Counts of Arms Pulled: {bandit.arm_counts}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Reward: 881\n",
      "Estimated Values of Arms: [0.         0.         0.         0.33333333 0.88597376]\n",
      "Counts of Arms Pulled: [  0.   0.   0.   9. 991.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class SoftmaxBandit:\n",
    "    def __init__(self, n_arms, temperature, true_rewards):\n",
    "        self.n_arms = n_arms\n",
    "        self.temperature = temperature  # Temperature parameter for softmax\n",
    "        self.true_rewards = true_rewards  # True mean rewards for each arm\n",
    "        self.arm_counts = np.zeros(n_arms)  # Count of times each arm was pulled\n",
    "        self.arm_values = np.zeros(n_arms)  # Estimated value of each arm\n",
    "\n",
    "    def select_arm(self):\n",
    "        # Calculate the probabilities using the softmax formula\n",
    "        exp_values = np.exp(self.arm_values / self.temperature)\n",
    "        probabilities = exp_values / np.sum(exp_values)\n",
    "        \n",
    "        # Choose an arm based on the computed probabilities\n",
    "        return np.random.choice(self.n_arms, p=probabilities)\n",
    "\n",
    "    def update_estimates(self, chosen_arm, reward):\n",
    "        # Increment the count of the chosen arm\n",
    "        self.arm_counts[chosen_arm] += 1\n",
    "        \n",
    "        # Update the estimated value of the chosen arm using incremental formula\n",
    "        n = self.arm_counts[chosen_arm]\n",
    "        value = self.arm_values[chosen_arm]\n",
    "        # Update rule: Q_new = Q_old + (reward - Q_old) / n\n",
    "        self.arm_values[chosen_arm] = value + (reward - value) / n\n",
    "\n",
    "    def run(self, n_steps):\n",
    "        total_reward = 0\n",
    "        rewards = np.zeros(n_steps)\n",
    "        \n",
    "        for step in range(n_steps):\n",
    "            # Select an arm using softmax strategy\n",
    "            chosen_arm = self.select_arm()\n",
    "            \n",
    "            # Simulate the reward for the chosen arm based on the true reward distribution\n",
    "            reward = np.random.binomial(1, self.true_rewards[chosen_arm])\n",
    "            \n",
    "            # Update the estimates for the chosen arm\n",
    "            self.update_estimates(chosen_arm, reward)\n",
    "            \n",
    "            # Update total reward and store reward of this step\n",
    "            total_reward += reward\n",
    "            rewards[step] = reward\n",
    "        \n",
    "        return total_reward, rewards\n",
    "\n",
    "# Parameters\n",
    "n_arms = 5                     # Number of arms\n",
    "temperature = 0.1              # Temperature parameter for softmax\n",
    "true_rewards = [0.1, 0.5, 0.7, 0.3, 0.9]  # True probabilities of reward for each arm\n",
    "n_steps = 1000                 # Number of steps\n",
    "\n",
    "# Create an instance of the bandit with softmax strategy\n",
    "bandit = SoftmaxBandit(n_arms, temperature, true_rewards)\n",
    "\n",
    "# Run the bandit algorithm for the specified number of steps\n",
    "total_reward, rewards = bandit.run(n_steps)\n",
    "\n",
    "# Output the results\n",
    "print(f\"Total Reward: {total_reward}\")\n",
    "print(f\"Estimated Values of Arms: {bandit.arm_values}\")\n",
    "print(f\"Counts of Arms Pulled: {bandit.arm_counts}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
